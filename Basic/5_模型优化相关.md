# 模型优化相关

------------------

## `Learning Rate`的选择依赖于选择的优化器

&emsp; 对于`Adam Optimizer`，我们通常选择初始学习率为`lr = 3e-4`，且`Adam Optimizer`已经内置了自动进行学习率的衰减更新，不需要人为更新。

-----------------

## **批标准化**`Batch Normalization`的使用

[参考链接：知乎：什么是批标准化 (Batch Normalization)](https://zhuanlan.zhihu.com/p/24810318)

### 1. 每层都做标准化

&emsp; 在神经网络中，数据分布对训练会产生影响。比如某个神经元`x`的值为`1`，某个`Weights`的初始值为`0.1`，这样后一层神经元计算结果就是`Wx = 0.1`；又或者`x = 20`, 这样`Wx = 2`。

&emsp; 现在还不能看出什么问题，但是，当我们加上一层激励函数，激活这个`Wx`值的时候，问题就来了。如果使用像`tanh`的激励函数，`Wx`的激活值就变成了`~0.1`和`~1`，接近于`1`的部已经处在了激励函数的饱和阶段，也就是`x`无论再怎么扩大，`tanh`激励函数输出值也还是接近1。换句话说，神经网络在初始阶段已经不对那些比较大的`x`特征范围敏感了。这样很糟糕，想象我轻轻拍自己的感觉和重重打自己的感觉居然没什么差别，这就证明我的感官系统失效了。

&emsp; 当然我们是可以用之前提到的对数据做`normalization`预处理，使得输入的x变化范围不会太大，让输入值经过激励函数的敏感部分。但刚刚这个不敏感问题不仅仅发生在神经网络的输入层，而且在隐藏层中也经常会发生。
