# 模型优化相关

------------------

## `Learning Rate`的选择依赖于选择的优化器

&emsp; 对于`Adam Optimizer`，我们通常选择初始学习率为`lr = 3e-4`，且`Adam Optimizer`已经内置了自动进行学习率的衰减更新，不需要人为更新。

-----------------

## **批标准化**`Batch Normalization`的概念及使用

### 1. 概念：

[参考链接：知乎：什么是批标准化 (Batch Normalization)](https://zhuanlan.zhihu.com/p/24810318)

#### 1.1 每层都做标准化

&emsp; 在神经网络中，数据分布对训练会产生影响。比如某个神经元`x`的值为`1`，某个`Weights`的初始值为`0.1`，这样后一层神经元计算结果就是`Wx = 0.1`；又或者`x = 20`, 这样`Wx = 2`。

&emsp; 现在还不能看出什么问题，但是，当我们加上一层激励函数，激活这个`Wx`值的时候，问题就来了。如果使用像`tanh`的激励函数，`Wx`的激活值就变成了`~0.1`和`~1`，接近于`1`的部已经处在了激励函数的饱和阶段，也就是`x`无论再怎么扩大，`tanh`激励函数输出值也还是接近1。换句话说，神经网络在初始阶段已经不对那些比较大的`x`特征范围敏感了。这样很糟糕，想象我轻轻拍自己的感觉和重重打自己的感觉居然没什么差别，这就证明我的感官系统失效了。

&emsp; 当然我们是可以用之前提到的对数据做`normalization`预处理，使得输入的x变化范围不会太大，让输入值经过激励函数的敏感部分。但刚刚这个不敏感问题不仅仅发生在神经网络的输入层，而且在隐藏层中也经常会发生。

#### 1.2 标准化 + 反标准化

&emsp; `Batch Normalization`在利用均值和方差进行标准化之后，还会进行一次**反标准化**，将标准化后的数据进行扩展和平移，这是为了让神经网络自己去学着使用和修改这个**扩展参数**`gamma`, 和**平移参数**`β`，这样神经网络就能自己慢慢琢磨出前面的`normalization`操作到底有没有起到优化的作用, 如果没有起到作用, 我就使用`gamma`和`β`来抵消一些`normalization`的操作。


### 2. `BN`层的使用方法

#### 2.1 添加位置

&emsp; 加在卷积层或全连接层后面都可。[keras添加BatchNormalization层](https://blog.csdn.net/coffrelv/article/details/102657861)

#### 2.2 代码

```
update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
    optimizer = tf.train.AdamOptimizer(lr).minimize(loss) # 定义优化器
    
... ... # 省略其他部分

output = tf.layers.batch_normalization(inputs=output, training=True) # Batch Normalization层，注意在test时需要将`training`改为`False`

```

### 3. 注意事项

#### 3.1 目前已知的导致性能变差（`validation loss`上升）的情形

 - `CNN`模型简单（两层卷积层 + 两层池化层 + 一层全连接层），`BN`层加在全连接层后面。


-------------------

## `Early Stop`的判据问题

&emsp; 根据实验得出结论：使用`Validation loss`的上升作为判据要优于使用`AUC Score`的下降作为判据。


-------------------

## `Dropout`的使用

&emsp; 首先利用`placeholder`声明一个空间：
```
dropout_rate = tf.placeholder_with_default(0.0, shape=())
```

&emsp; 再在网络结构中加入`dropout`层，这里选择在全连接层后面加：












